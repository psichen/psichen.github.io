---
title: Common probability density function derivation
tags: [probability density function, poisson, exponential, gaussian]
mathjax: true
---

Probability and statistics are one of the most common terminologies in scientific data interpretations. They provide us ways to find truths and make conclusions from this stochastic and noisy world. These different probability distributions are heavily coupled. In order to apply these distribution properly, we should understand their origin and derivation, which are the main purpose of this post.

## Probability mass function for discrete variables

$X$: random variable

$p$: probability, $p\in(0,1)$

$q$: equals to $1-p$

$k$: discrete event numbers

$x$: continuous variables

$P(X)$: probability mass function

$f(x)$: probability density function

others: parameters

### Bernoulli distribution

$$ P(X=k) = p^k q^{1-k} $$

where $k=0, 1$.

Bernoulli distribution is associated with *one trial with two outcomes*, generally refered to as success ($k=1$) and failure ($k=0$).

#### probability generating function

#### mean and variance

$$ E(X) = p $$

$$ V(X) = pq $$ 

### Geometric distribution

A random variable $X$ is given by the number of trials needed to obtain the first success when the chance of sucess at each trial is constant and equals to $p$.

$$ P(X=k) = q^{k-1}p $$

where $k>0$ and is an integer number.

#### probability generating function

$$ \Phi_X(t) = \sum_{k=0}^{\infty} q^{k-1} p t^k = \frac{p}{q} \sum_{k=1}^{\infty} (qt)^k = \frac{pk}{1-qk} $$

where the sum of a geometric series was used.

#### mean and variance

$$ E(X) = \frac{1}{p} $$
$$ V(X) = \frac{q}{p^2} $$

### Negativee geometric distribution

### Hypergeometric distribution

### Binomial distribution

$$ P(X=k) = C(n, k)p^k(1-p)^{n-k} $$

where $k=0,1,\dots,n$.

#### probability generating function

### Poisson distribution

$$ P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!} $$

where $k = 0, 1, 2,\dots$

#### probability generating function

$$ \Phi_X(t) = \sum_{k=0}^{\infty} \frac{e^{-\lambda} \lambda^k}{k!} t^k = e^{\lambda(t-1)} $$

#### memoryless property

## Probability density function for continuous variables

### Gaussian distribution

$$ f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2} $$

#### moment generating function

#### Fourier transform

### Exponential distribution

$$ f(x) = \lambda e^{-\lambda x} $$

#### moment generating function

## Appendixes

### Probability generating functions for discrete variables

The probability generating function is defined as

$$ \Phi_X(t) = E[t^X] = \sum_{k=0}^{\infty} P(X=k) t^k  $$

which is a power series representation of the probability mass function of the random variables. The probability of random variable $X=k$ is generated by

$$ P(X=k) = \frac{\Phi_X^{(k)}(0)}{k!} $$

If random variables have equal probability generating functions, then they hace identical distributions.

Two important results (mean and variance) can be obtained by differentiating $\Phi_X(t)$ with respect to $t$. The first derivative is

$$ \frac{d\Phi_X(t)}{dt} = \sum_{k=0}^{\infty} kP(X=k)t^{k-1} $$

and the second derivative is

$$ \frac{d^2\Phi_X(t)}{dt^2} = \sum_{k=0}^{\infty} k(k-1)P(X=k)t^{k-2} $$

We assign $t=1$ and get

$$ \Phi'_X(1) = E(X) $$

$$ \Phi''_X(1) = E(X^2)-E(X) $$

So from derivates of $\Phi_X(t)$ we get

$$ V(X) = E(X^2)-[E(X)]^2 = \Phi''_X(1) + \Phi'_X(1) - [\Phi'_X(1)]^2 $$

For a multivariate discrete distribution, the original $\Phi_X(t)$ is defined as

$$ \begin{align} \Phi_{X+Y}(t) &= \sum_{k=0}^{\infty} P(X+Y=k)t^k \\ &= \sum_{k=0}^{\infty} t^k \sum_{r=0}^{r=k} P(X=r)P(Y=k-r) \\ &= \sum_{k=0}^{\infty}\sum_{r=0}^{r=k} P(X=r)t^rP(Y=k-r)t^{k-r} \\ &= \sum_{r=0}^{\infty}\sum_{k=r}^{\infty} P(X=r)t^rP(Y=k-r)t^{k-r} \end{align} $$

The change of summation order is analogous to the change of integration order

$$ \int_0^{\infty}dy \int_0^y dx = \int_0^{\infty} dx \int_x^{\infty} dy $$

Thus, the probability generating function of variable $X+Y$ is equal to the product of respective probability generating functions of $X$ and $Y$

$$ \Phi_{X+Y}(t) = \Phi_X(t) \Phi_Y(t) $$

### Moments generating functions

Moments generating functions are defined as

$$ M_X(t) = E[e^{Xt}] = \begin{cases} \sum_i e^{tx_i} P(X=x_i) & \text{for a discrete distribution} \\ \int e^{tx} f(x) dx & \text{for a continuous distribution} \end{cases} $$

The exponent can be expand as a power series as

$$ M_X(t) = E[e^{tX}] = E[\sum_{k=0}^{\infty} \frac{X^k}{k!} t^k] = \sum_{k=0}^{\infty} \frac{t^k}{k!} E[X^k] $$

So moments of different orders can generated from corresponding derivates of $M_X(t)$

$$ E[X^k] = M_X^{(k)}(0) $$

Similarly, for a multivariate $X+Y=N$ distribution, the moments generating function is defined as

$$ \begin{align} M_{X+Y}(t) &= E[e^{(X+Y)t}] \\ &= \int_{-\infty}^{\infty} e^{nt} f_{X+Y}(n) dn \\ &= \int_{-\infty}^{\infty} e^{nt} dn \int_{-\infty}^{\infty} f_X(x)f_Y(n-x) dx \\ &= \int_{-\infty}^{\infty} e^{xt}f_X(x) dx \int_{-\infty}^{\infty} e^{(n-x)t}f_Y(n-x) dn \\ &=E[e^{Xt}]E[e^{Yt}] \\ &= M_X(t)M_Y(t) \end{align} $$

which is the product of respective moments generating functions of independent variables $X$ and $Y$. This property is useful in the derivation of Poisson/Binomial distributions from Binomial/Bernoulli distributions.

### Characteristic functions



## References
[Univariate Distribution Relationships](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html)
