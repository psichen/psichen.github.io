<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="psichen">


  <meta name="subtitle" content="I have a proof of this theorem, but there is not enough space in this margin">




<title>Common probability density functions | pan.world</title>





<link rel="stylesheet" href="/lib/nprogress/nprogress.css">


<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>

<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>


<script src="/lib/nprogress/nprogress.js"></script>

<script>
  $(document).ready(() => {
    NProgress.configure({
      showSpinner: false,
    });
    NProgress.start();
    $("#nprogress .bar").css({
      background: "#de7441",
    });
    $("#nprogress .peg").css({
      "box-shadow": "0 0 2px #de7441, 0 0 4px #de7441",
    });
    $("#nprogress .spinner-icon").css({
      "border-top-color": "#de7441",
      "border-left-color": "#de7441",
    });
    setTimeout(function () {
      NProgress.done();
      $(".fade").removeClass("out");
    }, 800);
  });
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      isDark = !isDark;
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);
    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ri:moon-line" : "ri:sun-line"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>


<meta name="generator" content="Hexo 6.0.0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/node-tikzjax@latest/css/fonts.css" /><style>.tikzjax { display: block; text-align: center; user-select: none; }</style></head>
<body class="font-sans bg-white dark:bg-zinc-900 text-gray-700 dark:text-gray-200">
  <header class="fixed w-full px-5 py-1 z-10 backdrop-blur-xl backdrop-saturate-150 border-b border-black/5">
  <div class="max-auto">
    <nav class="flex items-center text-base">
      <a href="/" class="group">
        <h2 class="font-medium tracking-tighterp text-l p-2">
          pan.world
        </h2>
      </a>
      <div class="flex-1"></div>
      <div class="flex items-center gap-3">
        
          <a class="hidden sm:flex" href="/archives">Archives</a>
        
          <a class="hidden sm:flex" href="/tag">Tags</a>
        
          <a class="hidden sm:flex" href="/about">About</a>
        
        <a class="w-5 h-5" title="toggle theme" id="toggle-dark">
          <iconify-icon width="20" icon="" id="theme-icon"></iconify-icon>
        </a>
      </div>
      <div class="flex items-center justify-center gap-3 ml-3 sm:hidden">
        <span class="w-5 h-5" aria-hidden="true" role="img" id="open-menu">
          <iconify-icon width="20" icon="carbon:menu" ></iconify-icon>
        </span>
        <span class="w-5 h-5 hidden" aria-hidden="true" role="img" id="close-menu">
          <iconify-icon  width="20" icon="carbon:close" ></iconify-icon>
        </span>
      </div>
    </nav>
  </div>
</header>
<div id="menu-panel" class="h-0 overflow-hidden sm:hidden fixed left-0 right-0 top-12 bottom-0 z-10">
  <div id="menu-content" class="relative z-20 bg-white/80 px-6 sm:px-8 py-2 backdrop-blur-xl -translate-y-full transition-transform duration-300">
    <ul class="nav flex flex-col sm:flex-row text-sm font-medium">
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/archives" class="flex h-12 sm:h-auto items-center">Archives</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/tag" class="flex h-12 sm:h-auto items-center">Tags</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/about" class="flex h-12 sm:h-auto items-center">About</a>
        </li>
      
    </ul>
  </div>
  <div class="mask bg-black/20 absolute inset-0"></div>
</div>

  <main class="p-12">
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">


<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

<!-- toc -->

  <!-- tocbot -->
<nav class="post-toc toc text-sm w-48 relative top-32 right-0 opacity-70 hidden xl:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-4xl mx-auto md:px-0">
  <!-- header -->
  <header class="overflow-hidden pt-6 pb-6 md:pt-12">
    <div class="pt-4 md:pt-6">
      <h1 id="article-title" class="text-[2rem] font-bold leading-snug mb-4 md:mb-6 md:text-[2.6rem]">
        Common probability density functions
      </h1>
      <div>
        <section class="flex items-center gap-3 text-sm">
          <time>2021-03-10</time>
          <span class="text-gray-400">Â·</span>
          <span>2.7k words</span>
        </section>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content mt-10 prose-lg dark:prose-invert post-content">
    <p>Probability and statistics are one of the most common terminologies
in scientific data interpretations. They provide us ways to find truths
and make conclusions from this stochastic and noisy world. These
different probability distributions are heavily coupled. In order to
apply these distribution properly, we should understand their origin and
derivation, which are the main purpose of this post (for continuous
distributions).</p>
<span id="more"></span>
<h1 id="gaussian-distribution">Gaussian distribution</h1>
<p>Gaussian distribution is probably the most famous probability
distribution. It's defined as</p>
<p><span class="math display">\[ f(x) = \frac{1}{\sigma \sqrt{2\pi}}
e^{- \frac{1}{2}(\frac{x-\mu}{\sigma})^2} \]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean and <span
class="math inline">\(\sigma^2\)</span> is the variance.</p>
<h2 id="moment-generating-function">moment generating function</h2>
<p>The moment generating function of Gaussian distribution can be
directly calculated from the definition by just changing the integral to
the form <span class="math inline">\(a \int e^{-b(x-c)^2}
dx\)</span>,</p>
<p><span class="math display">\[ M_X(t) = E[e^{Xt}] = \frac{1}{\sigma
\sqrt{2\pi}} \int e^{- \frac{1}{2} (\frac{x-\mu}{\sigma})^2 + xt} dx =
e^{\mu t+\frac{1}{2} \sigma^2 t^2} \]</span></p>
<h2 id="central-limit-theorem">central limit theorem</h2>
<p>Suppose that <span class="math inline">\(X_i,~~i=1,2,\dots,n\)</span>
are independent random variables, each of which is described by a
probability density function <span class="math inline">\(f(x_i)\)</span>
(these functions may be all different) with a mean <span
class="math inline">\(\mu_i\)</span> and a variance <span
class="math inline">\(\sigma_i\)</span>. The variable <span
class="math inline">\(Z = \frac{\sum_i X_i}{n}\)</span> follows Gaussian
distribution when <span class="math inline">\(n \to \infty\)</span>,</p>
<p><span class="math display">\[ f(z) = \frac{1}{\sigma_z \sqrt{2\pi}}
e^{\frac{1}{2} (\frac{x-\mu_z}{\sigma_z})^2} \]</span></p>
<p>where <span class="math inline">\(\mu_z=\frac{\sum_i
\mu_i}{n}\)</span> and <span
class="math inline">\(\sigma_z^2=\frac{\sum_i
\sigma_i^2}{n^2}\)</span>.</p>
<p>Let's consider this property from moment generating function of every
random variable <span class="math inline">\(X_i\)</span>, namely, <span
class="math inline">\(M_{X_i}(t)\)</span>. So for <span
class="math inline">\(Z=\frac{\sum_i X_i}{n}\)</span>, we have</p>
<p><span class="math display">\[ M_Z(t) = \prod_i M_{X_i}(\frac{t}{n})
\]</span></p>
<p>Recall that <span class="math inline">\(M_{X_i}(\frac{t}{n})=E[e^{X_i
\frac{t}{n}}]\)</span> and expand the exponential component, we can
get</p>
<p><span class="math display">\[ \begin{aligned}
M_{X_i}(\frac{t}{n}) &amp;= 1 + \frac{t}{n} E[X_i] + \frac{1}{2!}
\frac{t^2}{n^2} E[X_i^2] + \cdots \\
                     &amp;= 1 + \frac{t}{n} \mu_i + \frac{1}{2!}
\frac{t^2}{n^2} (\mu_i^2+\sigma_i^2) + \cdots
\end{aligned} \]</span></p>
<p>When <span class="math inline">\(n \to \infty\)</span>,</p>
<p><span class="math display">\[ M_{X_i}(\frac{t}{n}) \approx
e^{\frac{\mu_i t}{n} + \frac{1}{2} \frac{\sigma_i^2 t^2}{n^2}} = (1 +
\frac{\mu_i t}{n} + \frac{1}{2} \frac{\mu^2 t^2}{n^2} +
O((\frac{t}{n})^3))(1 + \frac{1}{2} \frac{\sigma_i^2 t^2}{n^2} +
O((\frac{t}{n})^4)) \]</span></p>
<p>as approximated to the second term <span
class="math inline">\((\frac{t^2}{n^2})\)</span>. So the moment
generating function of <span class="math inline">\(Z\)</span> can be
easily gotten by the product,</p>
<p><span class="math display">\[ M_Z(t) = \prod_i M_{X_i}(\frac{t}{n}) =
e^{\frac{\sum_i \mu_i}{n}t + \frac{1}{2} \frac{\sum_i \sigma_i^2}{n^2}
t^2} \]</span></p>
<p>which is exactly the form of Gaussian distribution, regardless of the
type of probability distribution of random variables <span
class="math inline">\(X_i\)</span>.</p>
<h1 id="log-normal-distribution">Log-normal distribution</h1>
<p>If <span class="math inline">\(X \sim Gaussian(\mu,
\sigma^2)\)</span>, then the variable <span class="math inline">\(Y =
e^X\)</span> follows the log-normal distribution, and <span
class="math inline">\(\ln Y\)</span> follows the normal distribution,
hence the name. According to the <a
href="https://psichen.github.io/2021/02/04/geometric-interpretation-about-Jacobian-determinant/">Jacobian
determinant</a>, the probability density function of <span
class="math inline">\(Y\)</span> is the following equation,</p>
<p><span class="math display">\[ f(y) = f(x(y)) \frac{\partial
x}{\partial y} = \frac{1}{\sigma \sqrt{2\pi}} \frac{1}{y}
e^{-\frac{1}{2} (\frac{\ln y-\mu}{\sigma})^2} \]</span></p>
<p>The <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma^2\)</span> are not mean and variance but
parameters of log-normal distribution. the mean and variance of
log-normal distribution can be calculated from the moment generating
function of Gaussian distribution,</p>
<p><span class="math display">\[ \begin{aligned}
E[Y] &amp;= E[e^X] = M_X(1) = e^{\mu+\frac{1}{2}\sigma^2} \\
V[Y] &amp;= E[Y^2] - E^2[Y] = E[e^{2X}] - E^2[e^X] = M_X(2) - M_X^2(1)
\\
&amp;= e^{2\mu+2\sigma^2} - e^{2\mu+\sigma^2}
\end{aligned} \]</span></p>
<h2 id="log-normal-central-limit-theorem">Log-normal central limit
theorem</h2>
<p>Let's consider the variable <span class="math inline">\(Y=(\prod_i
X_i)^{\frac{1}{n}},~~i=1,2,\dots,n\)</span>. So the variable <span
class="math inline">\(\ln Y=\frac{1}{n} \sum_i^n \ln X_i\)</span> and
satisfies the central limit theorem when <span class="math inline">\(n
\to \infty\)</span>. In other words, the random variable <span
class="math inline">\(Y\)</span> follows the log-normal
distribution.</p>
<h1 id="exponential-distribution">Exponential distribution</h1>
<p>Exponential distribution describes the waiting time between two
successive independent events in a Poisson process. The probability
density function of exponential distribution can be derived from Poisson
distribution.</p>
<p>If the average number of events per unit interval is <span
class="math inline">\(\lambda\)</span>, then on average there are <span
class="math inline">\(\lambda x\)</span> events in interval <span
class="math inline">\(x\)</span> according to the sum property of
Poisson distributions. From the Poisson distribution the probability
that no event occurs in interval <span class="math inline">\(x\)</span>
is,</p>
<p><span class="math display">\[ P_0 = \frac{(\lambda x)^k}{k!}
e^{-\lambda x} |_{k=0} = e^{-\lambda x} \]</span></p>
<p>The probaility that an event occurs in the next infinitesimal
interval <span class="math inline">\(dx\)</span> is,</p>
<p><span class="math display">\[ P_1 = \frac{(\lambda dx)^k}{k!}
e^{-\lambda dx} |_{k=1} = \lambda dx e^{-\lambda dx} \approx \lambda dx
\]</span></p>
<p>So the probability that the first event occur in interval <span
class="math inline">\([x, x+dx]\)</span> is the product of <span
class="math inline">\(P_0\)</span> and <span
class="math inline">\(P_1\)</span>,</p>
<p><span class="math display">\[ f(x) = \lambda e^{-\lambda x} dx
\]</span></p>
<p>which is the probability density function of exponential distribution
(<span class="math inline">\(x&gt;0\)</span>).</p>
<h2 id="moment-generating-function-1">moment generating function</h2>
<p>By definition,</p>
<p><span class="math display">\[ M_X(t) = E[e^{Xt}] = \lambda
\int_0^{\infty} e^{-\lambda x} e^{x t} dx = \frac{\lambda}{\lambda-t}
\]</span></p>
<p>where <span class="math inline">\(t&lt;\lambda\)</span>.</p>
<h2 id="memorylessness">memorylessness</h2>
<p>We say a random variable has memorylessness when it has following
property,</p>
<p><span class="math display">\[ P(X&gt;t+x|X&gt;t) = P(X&gt;x)
\]</span></p>
<p>For the Poisson process, its waiting time satifies exponential
distribution. According to the conditional probability rule, we have</p>
<p><span class="math display">\[ \begin{aligned}
P(X&gt;t+x|X&gt;t) &amp;= \frac{P((X&gt;t+x) \cap (X&gt;t))}{P(X&gt;t)}
= \frac{P(X&gt;t+x)}{P(X&gt;t)} \\
             &amp;= \frac{\int_{t+x}^{\infty} \lambda e^{-\lambda x}
dx}{\int_t^{\infty} \lambda e^{-\lambda x}dx} = \frac{e^{-\lambda
(t+x)}}{e^{-\lambda t}} \\
             &amp;= e^{-\lambda x} = \int_x^{\infty} \lambda e^{-\lambda
x} dx = P(X&gt;x)
\end{aligned} \]</span></p>
<p>An application of memorylessness is to join two signal series of
Poisson processes. Signal series consist of a series of discrete values.
For simplicity, every value belongs to some states. In order to avoid
artifacts, we should select such signal series of which the state of the
tail of the first signal series should be the same as the state of the
head of the second signal series. Then the tail of the first signal
series is superposed with the head of the second signal series. Because
of the memorylessness, the state transition probability of the tail of
the first signal series should be the same as that of the head of the
second signal series. This join way keeps the stochastic property around
the joint point.</p>
<h1 id="gamma-distribution">Gamma distribution</h1>
<p>Exponential distribution can be extended to Gamma distribution by
considering the waiting time when <span class="math inline">\(r\)</span>
events occur. The probability density function of Gamma function can be
derived by considering Poisson distribution where <span
class="math inline">\(r-1\)</span> events occur in interval <span
class="math inline">\(x\)</span>,</p>
<p><span class="math display">\[ P_{r-1} = \frac{(\lambda
x)^{r-1}}{(r-1)!} e^{-\lambda x} \]</span></p>
<p>Similarly, the probability that <span
class="math inline">\(r\)</span>th event occurs in interval <span
class="math inline">\([x, x+dx]\)</span> is</p>
<p><span class="math display">\[ P_r = \frac{(\lambda x)^{r-1}}{(r-1)!}
e^{-\lambda x} \lambda dx \]</span></p>
<p>Thus, the probability density function of Gamma distribution is</p>
<p><span class="math display">\[ f(x) = \frac{\lambda (\lambda
x)^{r-1}}{(r-1)!} e^{-\lambda x} \]</span></p>
<p>The gamma distribution can be defined for all positive real number
<span class="math inline">\(r\)</span> by replacing <span
class="math inline">\((r-1)!\)</span> by Gamma function <span
class="math inline">\(\Gamma(r)\)</span>,</p>
<p><span class="math display">\[ f(x) = \frac{\lambda^r
x^{r-1}}{\Gamma(r)} e^{-\lambda x} \]</span></p>
<p>and we write <span class="math inline">\(X \sim Gamma(r,
\lambda)\)</span> if the random variable <span
class="math inline">\(X\)</span> follows Gamma distributon with the
shape parameter <span class="math inline">\(r\)</span> and the rate
parameter <span class="math inline">\(\lambda\)</span>.</p>
<h2 id="moment-generating-function-2">moment generating function</h2>
<p>The moment generating function for gamma function is obtained from
that for exponential distribution,</p>
<p><span class="math display">\[ M_X(t) = (\frac{\lambda}{\lambda-t})^r
\]</span></p>
<p>In other words, exponential distribution is a special case of gamma
distribution <span class="math inline">\(Gamma(\lambda, 1)\)</span>.</p>
<h2 id="continuation-from-poisson-distribution">continuation from
Poisson distribution</h2>
<p><a
href="https://psichen.github.io/2021/02/13/common-probability-mass-functions/">Poisson
distribution</a> describes the probability of the number of independent
random events occurring. It's a discrete distribution in which the
random variable must be integers. We can extend the Poisson distribution
from integer space to real space by replacing factorial <span
class="math inline">\(k!\)</span> with gamma function <span
class="math inline">\(\Gamma(k+1)\)</span>,</p>
<p><span class="math display">\[ P(x=k) = \frac{\lambda^k
e^{-\lambda}}{\Gamma(k+1)} = Gamma(1, k+1) \]</span></p>
<h1 id="beta-distribution">Beta distribution</h1>
<p>Beta distribution describes the probability distribution of a
probability. We can derive the beta distribution via Bayesian
inference.</p>
<p>Let's first consider the probability of an event occuring <span
class="math inline">\(p\)</span>. We do not know the exact value of the
<span class="math inline">\(p\)</span>, so we just have a probability
distribution of <span class="math inline">\(p\)</span> in which some
values are more likely. When we don't have any sampling information
about the <span class="math inline">\(p\)</span>, we have no expectation
but rather randomly guess what the <span
class="math inline">\(p\)</span> is. So the prior probability <span
class="math inline">\(P(p&lt;x&lt;p+\Delta p)\)</span> is uniform
distribution <span class="math inline">\(f(p)=1\)</span> times <span
class="math inline">\(\Delta p\)</span> (shorthand for <span
class="math inline">\(P(p)\)</span>).</p>
<p>Then we can add new information to the <span
class="math inline">\(p\)</span> by counting success trials and failure
trials according to the Bayesian equation,</p>
<p><span class="math display">\[ P(p|X) = \frac{P(X|p)P(p)}{\sum_{p}
P(X|p) P(p)} \]</span></p>
<p>where <span class="math inline">\(P(p)\)</span> is the prior, <span
class="math inline">\(P(p|X)\)</span> is the posterior, <span
class="math inline">\(P(X|p)\)</span> is the likelyhood and <span
class="math inline">\(\sum_{p} P(X|p) P(p)\)</span> is the normalization
factor. We can derive the posterior probability mass function as,</p>
<p><span class="math display">\[ P(p|X) = \frac{ {n \choose k} p^k
(1-p)^{n-k} \Delta p}{\int_0^1 {n \choose k} p^k (1-p)^{n-k} dp} =
\frac{p^k (1-p)^{n-k} \Delta p}{\int_0^1 p^k (1-p)^{n-k} dp}
\]</span></p>
<p>Thus its probability density function is,</p>
<p><span class="math display">\[ f(x) = \frac{ x^{m-1}(1-x)^{n-1} }{
B(m,n) } \]</span></p>
<h2 id="moment-generating-function-3">moment generating function</h2>
<p>The moment generating function of Beta distribution can be obtained
by definition,</p>
<p><span class="math display">\[\begin{aligned}
M_X(t) &amp;= E[e^{Xt}] = \int_0^1 e^{xt}
\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)} dx \\
       &amp;= \int_0^1 (\sum_{k=0}^{\infty} \frac{t^k x^k}{k!})
\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)} dx \\
       &amp;= \frac{1}{B(\alpha, \beta)} \sum_{k=0}^{\infty}
\frac{t^k}{k!} \int_0^1 x^{\alpha+k-1}(1-x)^{\beta-1} dx \\
       &amp;= \sum_{k=0}^{\infty} \frac{t^k}{k!} \frac{B(\alpha+k,
\beta)}{B(\alpha, \beta)} \\
       &amp;= \sum_{k=0}^{\infty} \frac{t^k}{k!}
\frac{\Gamma(\alpha+k)\Gamma(\beta)}{\Gamma(\alpha+k+\beta)}
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \\
       &amp;= 1+\sum_{k=1}^{\infty} \frac{t^k}{k!} (\prod_{r=0}^{k-1}
\frac{\alpha+r}{\alpha+\beta+r}) \\
\end{aligned}\]</span></p>
<h2 id="conjugate-prior-probability">conjugate prior probability</h2>
<p>A prior distribution is called the conjugate prior distribution when
the type of the posterior distribution is the same with that of the
prior distribution after Bayesian inference. For example, Beta
distribution is the conjugate prior distribution to binomial
distribution. Let's consider the prior distribution,</p>
<p><span class="math display">\[ P(p) = \int_p^{p+\Delta p}
\frac{x^{m-1}(1-x)^{n-1}}{B(m,n)} dx \]</span></p>
<p>The posterior distribution is,</p>
<p><span class="math display">\[\begin{aligned}
P(p|X) &amp;= \frac{ {n \choose k} p^k(1-p)^{n-k} \int_p^{p+\Delta p}
\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)} dx }{ \int_0^1  {n
\choose k} p^k(1-p)^{n-k} \int_p^{p+\Delta p}
\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)} dx } \\
       &amp;= \frac{ p^{\alpha-1+k}(1-p)^{\beta-1+n-k} \Delta p }{
\int_0^1 p^{\alpha-1+k}(1-p)^{\beta-1+n-k} dp } \\
\end{aligned}\]</span></p>
<p>So the probability density function of the posterior distribution
is,</p>
<p><span class="math display">\[ f(x) = \frac{
x^{\alpha+k-1}(1-x)^{\beta+n-k-1} }{ B(\alpha+k, \beta+n-k) }
\]</span></p>
<p>it's just the sum of parameters of Beta and binomial
distributions.</p>
<h1 id="chi-squared-distribution">Chi-squared distribution</h1>
<p>If <span class="math inline">\(X \sim Gaussian(\mu,
\sigma^2)\)</span>, then the variable <span
class="math inline">\(Y=\frac{(X-\mu)^2}{\sigma^2}\)</span> is
distributed as <span class="math inline">\(Gamma(\frac{1}{2},
\frac{1}{2})\)</span>.</p>
<p>Because <span class="math inline">\(X=\mu \pm \sigma
\sqrt{Y}\)</span>, according to <a
href="https://psichen.github.io/2021/02/04/geometric-interpretation-about-Jacobian-determinant/">Jacobian
determinant</a>, the probability density function of <span
class="math inline">\(Y\)</span> needs sum of two parts,</p>
<p><span class="math display">\[ \begin{aligned}
f(y) &amp;= \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{y}{2}} |\frac{d
(\mu+\sigma \sqrt{y})}{dy}| + \frac{1}{\sigma \sqrt{2\pi}}
e^{-\frac{y}{2}} |\frac{d (\mu-\sigma \sqrt{y})}{dy}| \\
     &amp;= \frac{1}{\sqrt{2 \pi}}
e^{-\frac{y}{2}}  (\frac{y}{2})^{-\frac{1}{2}} = \Gamma(\frac{1}{2},
\frac{1}{2}) \\
\end{aligned} \]</span></p>
<p>Now let's consider <span class="math inline">\(n\)</span> independent
Gaussian random variables <span class="math inline">\(X_i \sim
Gaussian(\mu_i, \sigma_i^2)\)</span> and define <span
class="math inline">\(\chi_n^2 = \sum_{i=1}^{n}
\frac{(x_i-\mu_i)^2}{\sigma_i^2}\)</span>. The probability density
function of <span class="math inline">\(\chi_n^2\)</span> is</p>
<p><span class="math display">\[ f(\chi_n^2) =
(\frac{1}{2})^{\frac{1}{2}n}
\frac{(\chi_n^2)^{\frac{1}{2}n-1}}{\Gamma(\frac{1}{2}n)}
e^{-\frac{1}{2}\chi_n^2} \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the freedom
degree.</p>
<h2 id="moment-generating-function-4">moment generating function</h2>
<p>The probability density function of Chi-squared distribution can be
easily obtained from the moment generating function. Because <span
class="math inline">\(\chi_n^2 = \sum_{i=1}^{n}
\frac{(x_i-\mu_i)^2}{\sigma_i^2}\)</span>, the moment generating
function of <span class="math inline">\(\chi_n^2\)</span> is</p>
<p><span class="math display">\[ M_{\chi_n^2}(t) =
(\frac{\lambda}{\lambda-t})^{\frac{1}{2} n} \]</span></p>
<p>Thus <span class="math inline">\(\chi_n^2 \sim Gamma(\frac{1}{2},
\frac{1}{2}n)\)</span>.</p>
<h1 id="appendix">Appendix</h1>
<p>The gamma function and other related special functions appear in
probability density functions many times. Some properties of these
special functions need to be metioned.</p>
<h2 id="gamma-function">Gamma function</h2>
<p>The gamma function is defined as</p>
<p><span class="math display">\[ \Gamma(r) = \int_0^{\infty} t^{r-1}
e^{-t} dt \]</span></p>
<p>Let's make a substitution <span class="math inline">\(t=\lambda
x\)</span> and transform the equation, we can get the probability
density funtion of the gamma distribution,</p>
<p><span class="math display">\[ f(x) = \frac{\lambda^r x^{r-1}
e^{-\lambda x}}{\Gamma(r)} \]</span></p>
<h2 id="beta-function">Beta function</h2>
<p>The beta function is defined as</p>
<p><span class="math display">\[ B(m, n) = \int_0^1 x^{m-1} (1-x)^{n-1}
dx \]</span></p>
<p>We transfrom the equation and get the probability density function of
the beta distribution,</p>
<p><span class="math display">\[ f(x) =
\frac{x^{m-1}(1-x)^{n-1}}{B(m,n)} \]</span></p>
<p>Also,</p>
<p><span class="math display">\[ B(m, n) = \frac{\Gamma(m)
\Gamma(n)}{\Gamma(m+n)} \]</span></p>
<p>Gamma function is the continuation of factorial function
(permutation). Similarly, Beta function is the continuation of inverse
of combination.</p>
<p>If <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are independent random variables of
Gamma distribution, then the variable <span
class="math inline">\(Z=\frac{X}{X+Y}\)</span> obeys Beta distribution.
Let's define another variable <span class="math inline">\(W =
X+Y\)</span>, thus we have the probability density function <span
class="math inline">\(f(w, z)\)</span>,</p>
<p><span class="math display">\[ f(w, z) = f(x, y) \frac{(\partial x,
\partial y)}{(\partial w, \partial z)} \]</span></p>
<p>where <span class="math inline">\(\frac{(\partial x, \partial
y)}{(\partial w, \partial z)}\)</span> is the Jacobian determinant.</p>
<p>Because <span class="math inline">\(w=x+y, z=\frac{x}{x+y}\)</span>,
so <span class="math inline">\(x=wz, y=w(1-z)\)</span>. The Jacobian
determinant is,</p>
<p><span class="math display">\[ \begin{vmatrix} \frac{\partial
x}{\partial w} &amp; \frac{\partial y}{\partial w} \\ \frac{\partial
x}{\partial z} &amp; \frac{\partial y}{\partial z} \\ \end{vmatrix} =
\begin{vmatrix} z &amp; 1-z \\ w &amp; -w \\ \end{vmatrix} = -w
\]</span></p>
<p>we just take the absolute value of the Jacobian determinant because
it's <a
href="https://psichen.github.io/2021/02/04/geometric-interpretation-about-Jacobian-determinant/">the
scale factor of the infinitesimal area</a>. Thus we have,</p>
<p><span class="math display">\[\begin{aligned}
f(w, z) &amp;= \frac{\lambda^{\alpha}(wz)^{\alpha-1}e^{-\lambda
wz}}{\Gamma(\alpha)} \frac{\lambda^{\beta}(w(1-z))^{\beta-1}e^{-\lambda
w(1-z)}}{\Gamma(\beta)}w \\
        &amp;= \frac{\lambda^{\alpha+\beta} w^{\alpha+\beta-1}
e^{-\lambda w}}{\Gamma(\alpha+\beta)} \frac{z^{\alpha-1}
(1-z)^{\beta-1}}{B(\alpha, \beta)} \\
        &amp;= Gamma(\alpha+\beta, \lambda) \cdot Beta(\alpha, \beta) \\
\end{aligned}\]</span></p>
<p>it tells us the sum <span class="math inline">\(X+Y\)</span> and the
fraction <span class="math inline">\(\frac{X}{X+Y}\)</span> are
independent.</p>
<h1 id="references">References</h1>
<p><a
target="_blank" rel="noopener" href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html">Univariate
Distribution Relationships</a></p>

  </article>
  <!-- tag -->
  <div class="mt-12 pt-6 border-t border-gray-200">
    
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/probability-density-function/">probability density function</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/exponential/">exponential</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/gaussian/">gaussian</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/gamma/">gamma</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/beta/">beta</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/chi-squared/">chi-squared</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/log-normal/">log-normal</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/bayesian/">bayesian</a>
        </span>
      
    
  </div>
  <!-- prev and next -->
  <div class="flex justify-between mt-12 pt-6 border-t border-gray-200">
    <div>
      
        <a href="/2021/03/27/notes-about-fluorescence-correlation-spectroscopy-formalism/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          <iconify-icon width="20" icon="ri:arrow-left-s-line" data-inline="false"></iconify-icon>
          Notes about fluorescence correlation spectroscopy formalism
        </a>
      
    </div>
    <div>
      
        <a href="/2021/02/13/common-probability-mass-functions/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          Common probability mass functions
          <iconify-icon width="20" icon="ri:arrow-right-s-line" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->


  </main>
  <footer class="flex flex-col h-40 items-center justify-center text-gray-400 text-sm">
  <!-- copyright -->
  <div class="flex items-center gap-2">
    <span>Â© 2019 - 2024</span>
    <iconify-icon width="18" icon="eos-icons:atom-electron" ></iconify-icon>
    <span>psichen</span>
  </div>
  <div class="flex items-center gap-2">
    <span>
    Powered by
    <a href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>.
    Theme modified from
    <a href="https://github.com/xbmlz/hexo-theme-maple" target="_blank" rel="noopener noreferrer">Maple</a>.
    </span>
  </div>

</footer>

  <iconify-icon class="fixed right-4 bottom-10 z-100 opacity-0" width="24" icon="ion:arrow-up-c" id="go-top"></iconify-icon>
  
<script src="/js/main.js"></script>


<link rel="stylesheet" href="/css/main.css">


</body>

</html>
